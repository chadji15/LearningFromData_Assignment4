{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t9zBE2uci2Iu"
      },
      "outputs": [],
      "source": [
        "import random as python_random\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay, CosineDecayRestarts\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKmYG--YjM6y",
        "outputId": "ce970370-7fa5-4ea3-8f07-3a3ea58d5648"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUurccFRi6Hm",
        "outputId": "e78098cc-d4a6-42e3-93c0-17af98df9e02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(corpus_file):\n",
        "    \"\"\"Read in review data set and returns docs and labels\"\"\"\n",
        "    documents = []\n",
        "    labels = []\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip()\n",
        "            documents.append(\" \".join(tokens.split()[:-1]).strip())\n",
        "            labels.append(tokens.split()[-1])\n",
        "    return documents, labels"
      ],
      "metadata": {
        "id": "WIENeo6vi-u6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the data and embeddings\n",
        "X_train, Y_train = read_corpus('/content/gdrive/MyDrive/Colab Notebooks/ja/train.tsv')\n",
        "X_dev, Y_dev = read_corpus('/content/gdrive/MyDrive/Colab Notebooks/ja/dev.tsv')\n",
        "X_test, Y_test = read_corpus('/content/gdrive/MyDrive/Colab Notebooks/ja/test.tsv')\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "\n",
        "# Transform string labels to one-hot encodings\n",
        "encoder = LabelBinarizer()\n",
        "Y_train_bin = encoder.fit_transform(Y_train)  # Use encoder.classes_ to find mapping back\n",
        "Y_dev_bin = encoder.fit_transform(Y_dev)\n",
        "Y_test_bin = encoder.fit_transform(Y_test)\n",
        "labels = encoder.classes_\n",
        "\n",
        "# Transform the labels so it can be finetuned properly\n",
        "Y_train_bin = np.hstack((1 - Y_train_bin, Y_train_bin))"
      ],
      "metadata": {
        "id": "vTZQql1fjC3L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_word_frequency(X_train_vect, vocabulary):\n",
        "  ''' Compute word frequency in the given vocabulary'''\n",
        "  word_counts = {word: 0 for word in vocabulary}\n",
        "  for sentence in X_train_vect:\n",
        "    for word_index in sentence:\n",
        "      if word_index != 0: # If the word exists in the vocabulary increment the occurence count\n",
        "        word_counts[vocabulary[word_index]] += 1\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "CpZuAftijZ_i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_word_frequency_in_offensive_instances(X_train_vect, y_pred_test, vocabulary):\n",
        "  ''' Compute the word frequency in offensive samples'''\n",
        "  total_offensive_classifications = sum([1 if predicted_label == [1] else 0 for predicted_label in y_pred_test])\n",
        "  word_counts = {word: 0 for word in vocabulary}\n",
        "  for (sentence, predicted_label) in zip(X_train_vect, y_pred_test):\n",
        "    if predicted_label == [1]: # If the data instance is predicted as offensive add it's o\n",
        "      for word_index in sentence:\n",
        "        if word_index != 0:\n",
        "          word_counts[vocabulary[word_index]] += (1)\n",
        "  return {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "metadata": {
        "id": "MVOa5X42jhQN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_normalized_word_frequency_in_offensive_instances(word_frequency, word_frequency_in_offensive_instances, voc, support_threshold):\n",
        "  normalized_word_frequency_in_offensive_instances = {}\n",
        "  for word in voc:\n",
        "    if word_frequency[word] != 0 and word_frequency[word] >= support_threshold:\n",
        "      normalized_word_frequency_in_offensive_instances[word] = word_frequency_in_offensive_instances[word] / word_frequency[word]\n",
        "  return {k: v for k, v in sorted(normalized_word_frequency_in_offensive_instances.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "metadata": {
        "id": "ze8gmqeEjiS-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_offensiveness_metric(X, y_pred, voc):\n",
        "  word_frequency = compute_word_frequency(X, voc)\n",
        "  word_frequency_in_offensive_instances = compute_word_frequency_in_offensive_instances(X, y_pred, voc)\n",
        "  normalized_word_frequency_in_offensive_instances = compute_normalized_word_frequency_in_offensive_instances(word_frequency, word_frequency_in_offensive_instances, voc, 10)\n",
        "  return normalized_word_frequency_in_offensive_instances"
      ],
      "metadata": {
        "id": "xXmORcr3jjxu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_based_on_word_list(X, word_list):\n",
        "  y_pred = []\n",
        "  for sentence in X:\n",
        "    prediction = [0]\n",
        "    for word_token in sentence:\n",
        "      if word_token in word_list:\n",
        "        prediction = [1]\n",
        "    y_pred.append(prediction)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "I6TYwqdXjkfu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(lm, tokens_train, Y_train_bin, num_labels, epochs, batch_size, learning_rate):\n",
        "    ''' Train the model '''\n",
        "    print(\"Loading model....\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(lm, num_labels=num_labels)\n",
        "    loss_function = BinaryCrossentropy(from_logits=True)\n",
        "    num_decay_steps = len(Y_train_bin) * epochs\n",
        "    if learning_rate == \"PolynomialDecay\":\n",
        "        lr_scheduler = PolynomialDecay(\n",
        "            initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_decay_steps\n",
        "        )\n",
        "    elif learning_rate == \"CosineDecay\":\n",
        "        lr_scheduler = CosineDecay(\n",
        "            initial_learning_rate=5e-5, decay_steps = num_decay_steps\n",
        "        )\n",
        "    else:\n",
        "        lr_scheduler = learning_rate\n",
        "    optim = Adam(learning_rate=lr_scheduler)\n",
        "    print(\"Training model....\")\n",
        "    model.compile(loss=loss_function, optimizer=optim, metrics=['accuracy'])\n",
        "    model.fit(tokens_train, Y_train_bin, verbose=1, epochs=epochs,\n",
        "              batch_size=batch_size)\n",
        "    print(\"Done!\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "wH_F63f9j1dk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(lm, tokens_dev, Y_dev_bin, labels):\n",
        "    ''' Evaluate the model on the dev set'''\n",
        "    print(\"Evaluating model....\")\n",
        "    pred = lm.predict(tokens_dev)[\"logits\"]\n",
        "    # Get predictions using the trained model\n",
        "    # Finally, convert to numerical labels to get scores with sklearn\n",
        "    Y_pred = np.argmax(pred, axis=1)\n",
        "    # If you have gold data, you can calculate accuracy\n",
        "    Y_test = np.argmax(Y_dev_bin, axis=1)\n",
        "\n",
        "    report = classification_report(Y_test, Y_pred, target_names=labels, digits=3)\n",
        "    print(report)\n",
        "    cm = confusion_matrix(Y_test, Y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot()\n",
        "    return accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred, average='macro')"
      ],
      "metadata": {
        "id": "u3L50fCbj2iF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_param_grid():\n",
        "    ''' Create a parameter grid '''\n",
        "    param_grid = {'epochs': [1, 2, 3], 'max_seq_len': [50, 100, 150],\n",
        "                  'batches': [16, 32, 64],\n",
        "                  'lr_schedulers': [\"PolynomialDecay\", \"CosineDecay\", 5e-5, 3e-5]}\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    result = [dict(zip(keys, p)) for p in product(*values)]\n",
        "    return result"
      ],
      "metadata": {
        "id": "Yuy0PQdikFEV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the grid search\n",
        "lm = 'bert-base-uncased'\n",
        "param_grid = create_param_grid()\n",
        "performances = []\n",
        "i = 0\n",
        "seeds = [1234]\n",
        "for seed in seeds:\n",
        "    for parameters in param_grid:\n",
        "        tf.keras.backend.clear_session()\n",
        "        np.random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "        python_random.seed(seed)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(lm)\n",
        "        tokens_train = tokenizer(X_train, padding=True, max_length=parameters['max_seq_len'],\n",
        "                                  truncation=True, return_tensors=\"np\").data\n",
        "        tokens_dev = tokenizer(X_dev, padding=True, max_length=parameters['max_seq_len'],\n",
        "                                truncation=True, return_tensors=\"np\").data\n",
        "        tokens_test = tokenizer(X_test, padding=True, max_length=parameters['max_seq_len'],\n",
        "                                truncation=True, return_tensors=\"np\").data\n",
        "        model = train_model(lm, tokens_train, Y_train_bin,  len(labels),\n",
        "                            epochs=parameters['epochs'], batch_size=parameters['batches'], learning_rate=parameters['lr_schedulers'])\n",
        "\n",
        "        print(parameters)\n",
        "        acc, f1 = evaluate_model(model, tokens_dev, Y_dev_bin, labels)\n",
        "        performances.append(acc)\n",
        "        if i == 0:\n",
        "            parameters['accuracy'] = [acc]\n",
        "            parameters['f1'] = [f1]\n",
        "        else:\n",
        "            parameters['accuracy'].append(acc)\n",
        "            parameters['f1'].append(f1)\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "OGSFAEg1kKd2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_best_model():\n",
        "  lm = 'bert-base-uncased'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(lm)\n",
        "  tokens_train = tokenizer(X_train, padding=True, max_length=100,\n",
        "                            truncation=True, return_tensors=\"np\").data\n",
        "  tokens_dev = tokenizer(X_dev, padding=True, max_length=100,\n",
        "                          truncation=True, return_tensors=\"np\").data\n",
        "  tokens_test = tokenizer(X_test, padding=True, max_length=100,\n",
        "                          truncation=True, return_tensors=\"np\").data\n",
        "  model = train_model(lm, tokens_train, Y_train_bin,  len(labels),\n",
        "                      epochs=2, batch_size=64, learning_rate=3e-5)\n",
        "  return model"
      ],
      "metadata": {
        "id": "ubj5glH7pzhB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = return_best_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDVi0YckqA7Y",
        "outputId": "baf060f8-2095-4c1b-c845-51dcf32d4cfc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model....\n",
            "Epoch 1/2\n",
            "192/192 [==============================] - 249s 1s/step - loss: 0.4874 - accuracy: 0.7696\n",
            "Epoch 2/2\n",
            "192/192 [==============================] - 230s 1s/step - loss: 0.3756 - accuracy: 0.8380\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorization(standardize=None, output_sequence_length=50)\n",
        "# Use train and dev to create vocab - could also do just train\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train + X_dev)\n",
        "vectorizer.adapt(text_ds)\n",
        "# Dictionary mapping words to idx\n",
        "voc = vectorizer.get_vocabulary()\n",
        "# Transform input to vectorized input\n",
        "X_train_vect = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_val_vect = vectorizer(np.array([[s] for s in X_dev])).numpy()\n",
        "X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ],
      "metadata": {
        "id": "gk5FRqKFqfhh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores_test_set = []\n",
        "f1_scores_test_set = []\n",
        "\n",
        "for i in range(0, 10):\n",
        "  # Compute regular predictions\n",
        "  model = return_best_model()\n",
        "  y_pred_test_best_raw = model.predict(X_test_vect)\n",
        "  y_pred_test_best = [[1] if n > 0.5 else [0] for [n] in model.predict(X_test_vect)]\n",
        "  y_pred_test_best_f1 = f1_score(Y_test_bin, y_pred_test_best, average='macro')\n",
        "\n",
        "  f1_scores_test_set.append(y_pred_test_best_f1)\n",
        "\n",
        "  # Compute offensiveness metric\n",
        "  y_pred_train = [[1] if n > 0.5 else [0] for [n] in model.predict(X_train_vect)]\n",
        "  normalized_word_frequency_in_offensive_instances_train = compute_offensiveness_metric(X_train_vect, y_pred_train, voc)\n",
        "  word_list_train_unfiltered = list(normalized_word_frequency_in_offensive_instances_train.items())\n",
        "  word_list_train = list(filter(lambda e: e[1] > 0.5, word_list_train_unfiltered))\n",
        "  word_list_train_keys = [e[0] for e in word_list_train]\n",
        "\n",
        "  # This is a bit hacky, but use the tokenizer to convert the words back to their original token indexes\n",
        "  word_list_tokens_train = [int(word[0]) for word in vectorizer(word_list_train_keys)]\n",
        "  y_pred_word_list = classify_based_on_word_list(X_test_vect, word_list_tokens_train)\n",
        "  y_pred_word_list_f1 = f1_score(Y_test_bin, y_pred_word_list, average='macro')\n",
        "\n",
        "  f1_scores_test_set.append(y_pred_word_list_f1)"
      ],
      "metadata": {
        "id": "cvPOxLrRqJ76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}