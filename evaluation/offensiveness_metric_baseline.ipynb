{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rikZ7R3dhMt",
        "outputId": "b75fdb81-7fbe-412a-f0e7-f223fbe85896"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "Iiq1-BH0doqM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(corpus_file):\n",
        "    \"\"\"Read in review data set and returns docs and labels\"\"\"\n",
        "    documents = []\n",
        "    labels = []\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip()\n",
        "            documents.append(\" \".join(tokens.split()[:-1]).strip())\n",
        "            labels.append(tokens.split()[-1])\n",
        "    return documents, labels"
      ],
      "metadata": {
        "id": "qmzjmVR8dby-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q_5MCGoIdX9y"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = read_corpus('/content/gdrive/MyDrive/University/learning_from_data/assignment_4/train.tsv')\n",
        "X_val, y_val = read_corpus('/content/gdrive/MyDrive/University/learning_from_data/assignment_4/dev.tsv')\n",
        "X_test, y_test = read_corpus('/content/gdrive/MyDrive/University/learning_from_data/assignment_4/test.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelBinarizer()\n",
        "y_train_bin = encoder.fit_transform(y_train)  # Use encoder.classes_ to find mapping back\n",
        "y_val_bin = encoder.fit_transform(y_val)\n",
        "y_test_bin = encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "M-nK43fEf6OK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TextVectorization(standardize=None, output_sequence_length=50)\n",
        "# Use train and dev to create vocab - could also do just train\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "x3Xq4HRQde4A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vect = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_val_vect = vectorizer(np.array([[s] for s in X_val])).numpy()\n",
        "X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ],
      "metadata": {
        "id": "djDhI6sFfIvp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()"
      ],
      "metadata": {
        "id": "zVcplOPKd-22"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_word_frequency(X_, vocabulary):\n",
        "  word_counts = {word: 0 for word in vocabulary}\n",
        "  for sentence in X_:\n",
        "    for word_index in sentence:\n",
        "      if word_index != 0: # If the word exists in the vocabulary increment the occurence count\n",
        "        word_counts[vocabulary[word_index]] += 1\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "AbT7uXiJd1Kt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequency = compute_word_frequency(X_train_vect, voc)"
      ],
      "metadata": {
        "id": "vtMqzzXcd4su"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_word_frequency_in_offensive_instances(X_, y_, vocabulary):\n",
        "  total_offensive_classifications = sum([1 if predicted_label == [1] else 0 for predicted_label in y_])\n",
        "  word_counts = {word: 0 for word in vocabulary}\n",
        "  for (sentence, predicted_label) in zip(X_, y_):\n",
        "    if predicted_label == [1]: # If the data instance is predicted as offensive add it's o\n",
        "      for word_index in sentence:\n",
        "        if word_index != 0:\n",
        "          word_counts[vocabulary[word_index]] += (1)\n",
        "  return {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "metadata": {
        "id": "cNjGifLafpDP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequency_in_offensive_instances = compute_word_frequency_in_offensive_instances(X_train_vect, y_train_bin, voc)"
      ],
      "metadata": {
        "id": "URy5u4KbfrZX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_normalized_word_frequency_in_offensive_instances(word_frequency_, word_frequency_in_offensive_instances_, vocabulary, support_threshold_):\n",
        "  normalized_word_frequency_in_offensive_instances_ = {}\n",
        "  for word in vocabulary:\n",
        "    if word_frequency_[word] != 0 and word_frequency_[word] >= support_threshold_:\n",
        "      normalized_word_frequency_in_offensive_instances_[word] = word_frequency_in_offensive_instances_[word] / word_frequency_[word]\n",
        "  return {k: v for k, v in sorted(normalized_word_frequency_in_offensive_instances_.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "metadata": {
        "id": "mVDda_FWgQVN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_word_frequency_in_offensive_instances = compute_normalized_word_frequency_in_offensive_instances(word_frequency, word_frequency_in_offensive_instances, voc, 10)"
      ],
      "metadata": {
        "id": "t9gYtRsygjWf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_offensiveness_metric(X_, y_pred_, vocabulary):\n",
        "  word_frequency_ = compute_word_frequency(X_, voc)\n",
        "  word_frequency_in_offensive_instances_ = compute_word_frequency_in_offensive_instances(X_, y_pred_, vocabulary)\n",
        "  normalized_word_frequency_in_offensive_instances_ = compute_normalized_word_frequency_in_offensive_instances(word_frequency_, word_frequency_in_offensive_instances_, vocabulary, 10)\n",
        "  return normalized_word_frequency_in_offensive_instances_"
      ],
      "metadata": {
        "id": "8toaVrqegtsR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_based_on_word_list(X, word_list):\n",
        "  y_pred = []\n",
        "  for sentence in X:\n",
        "    prediction = [0]\n",
        "    for word_token in sentence:\n",
        "      if word_token in word_list:\n",
        "        prediction = [1]\n",
        "    y_pred.append(prediction)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "p91Pv6qGhJEr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "off_metric = compute_offensiveness_metric(X_train_vect, y_train_bin, voc)\n",
        "word_list = list(off_metric.items())\n",
        "word_list_filtered = list(filter(lambda e: e[1] > 0.5, word_list))\n",
        "word_list_filtered_keys = [e[0] for e in word_list_filtered]\n",
        "word_list_indexes = [int(word[0]) for word in vectorizer(word_list_filtered_keys)]"
      ],
      "metadata": {
        "id": "JcuwBKMrhkUW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_word_list_test = classify_based_on_word_list(X_test_vect, word_list_indexes)\n",
        "y_pred_word_list_f1_test = f1_score(y_test_bin, y_pred_word_list_test, average='macro')\n",
        "y_pred_word_list_f1_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaIX-lnDh5Bp",
        "outputId": "f4cdef2e-a5c0-49e5-9ec8-6aab71585f72"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.63589952780044"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_word_list_val = classify_based_on_word_list(X_val_vect, word_list_indexes)\n",
        "y_pred_word_list_f1_val = f1_score(y_val_bin, y_pred_word_list_val, average='macro')\n",
        "y_pred_word_list_f1_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn8PjqguEOqg",
        "outputId": "ba067213-72e5-423e-c15e-58d91769b8e4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6422386363276588"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}